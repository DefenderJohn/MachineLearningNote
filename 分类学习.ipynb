{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 分类学习\n",
    "## 决策树 Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 绘制Mermaid图表的工具，来自 https://gist.github.com/MLKrisJohnson/2d2df47879ee6afd3be9d6788241fe99\n",
    "import base64\n",
    "from IPython.display import Image, display\n",
    "\n",
    "def mm_ink(graphbytes):\n",
    "  \"\"\"Given a bytes object holding a Mermaid-format graph, return a URL that will generate the image.\"\"\"\n",
    "  base64_bytes = base64.b64encode(graphbytes)\n",
    "  base64_string = base64_bytes.decode(\"ascii\")\n",
    "  return \"https://mermaid.ink/img/\" + base64_string\n",
    "\n",
    "def mm_display(graphbytes):\n",
    "  \"\"\"Given a bytes object holding a Mermaid-format graph, display it.\"\"\"\n",
    "  display(Image(url=mm_ink(graphbytes)))\n",
    "\n",
    "def mm(graph):\n",
    "  \"\"\"Given a string containing a Mermaid-format graph, display it.\"\"\"\n",
    "  graphbytes = graph.encode(\"ascii\")\n",
    "  mm_display(graphbytes)\n",
    "\n",
    "def mm_link(graph):\n",
    "  \"\"\"Given a string containing a Mermaid-format graph, return URL for display.\"\"\"\n",
    "  graphbytes = graph.encode(\"ascii\")\n",
    "  return mm_ink(graphbytes)\n",
    "  \n",
    "def mm_path(path):\n",
    "  \"\"\"Given a path to a file containing a Mermaid-format graph, display it\"\"\"\n",
    "  with open(path, 'rb') as f:\n",
    "    graphbytes = f.read()\n",
    "  mm_display(graphbytes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 概要\n",
    "决策树是主要用于分类和回归任务的数据结构，它由节点和连接这两部分组成。\n",
    " - 节点：每个节点都代表了一个数据标签\n",
    " - 连接：由父节点连接到子节点的标签特征。\n",
    "我们下面会用一个简单的样例来解释决策树。\n",
    "\n",
    "首先，我们需要一些数据，这些数据应该具备自己的数据标签和观察到的特征。这里我们以“学生是否通过考试”为案例，示例如下：\n",
    "| 学习时长(StudyDuration:H/M/L) | 是否复习(Reviewed:Y/N) | 睡眠质量(S(leep)Q(uality):G/A(verage)/P(oor)) | 是否通过考试(Pass/Fail) |\n",
    "|--------|--------|--------|-----------|\n",
    "| 高     | 是    | 好     | 是        |\n",
    "| 高     | 否    | 好     | 否        |\n",
    "| 中     | 是    | 一般   | 是        |\n",
    "| 低     | 否    | 差     | 否        |\n",
    "| 低     | 是    | 好     | 否        |\n",
    "| 低     | 是    | 一般   | 是        |\n",
    "| 中     | 否    | 差     | 否        |\n",
    "| 高     | 是    | 一般   | 是        |\n",
    "\n",
    "随后，我们需要选取**特征**和**目标变量**。\n",
    " - 特征：特征指的是我们用来进行判断决策的指标，比如上表中的学习时长，是否复习等\n",
    " - 目标变量：目标变量指的是我们需要预测的指标，只能有一个，就是上表里的是否通过考试\n",
    "接下来我们需要对这张数据表进行观察并构建一棵决策树。构建决策树时我们需要使用一些算法来确定应该以什么特征进行判断决策，但在这里先暂时忽略，就以最基础的直觉进行构建，如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://mermaid.ink/img/Z3JhcGggVEQKICAgIEFbU3R1ZHlEdXJhdGlvbl0gLS0+fEh8IEIxW1Jldmlld2VkXQogICAgQSAtLT58TXwgQjJbUmV2aWV3ZWRdCiAgICBBIC0tPnxMfCBCM1tSZXZpZXdlZF0KICAgIEIxIC0tPnxZfCBDMVtTUV0KICAgIEIxIC0tPnxOfCBDMltTUV0KICAgIEIyIC0tPnxZfCBDM1tTUV0KICAgIEIyIC0tPnxOfCBDNFtTUV0KICAgIEIzIC0tPnxZfCBDNVtTUV0KICAgIEIzIC0tPnxOfCBDNltTUV0KICAgIEMxIC0tPnxHfCBEMVtQYXNzXQogICAgQzEgLS0+fEF8IEQyW1Bhc3NdCiAgICBDMSAtLT58UHwgRDNbRmFpbF0KICAgIEMyIC0tPnxHfCBENFtGYWlsXQogICAgQzIgLS0+fEF8IEQ1W0ZhaWxdCiAgICBDMiAtLT58UHwgRDZbRmFpbF0KICAgIEMzIC0tPnxHfCBEN1tGYWlsXQogICAgQzMgLS0+fEF8IEQ4W1Bhc3NdCiAgICBDMyAtLT58UHwgRDlbRmFpbF0KICAgIEM0IC0tPnxHfCBEMTBbRmFpbF0KICAgIEM0IC0tPnxBfCBEMTFbRmFpbF0KICAgIEM0IC0tPnxQfCBEMTJbRmFpbF0KICAgIEM1IC0tPnxHfCBEMTNbRmFpbF0KICAgIEM1IC0tPnxBfCBEMTRbUGFzc10KICAgIEM1IC0tPnxQfCBEMTVbRmFpbF0KICAgIEM2IC0tPnxHfCBEMTZbRmFpbF0KICAgIEM2IC0tPnxBfCBEMTdbUGFzc10KICAgIEM2IC0tPnxQfCBEMThbRmFpbF0=\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with open('分类学习/决策树1.data', 'r', encoding='utf-8') as file:\n",
    "    mm(file.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在这里，我们展示了一个纯粹直觉构建的决策树图表。当我们拿到一个新的数据，比如下表时，就可以根据上面构建的决策树进行预测。\n",
    "| 学习时长 | 是否复习 | 睡眠质量 | 是否通过考试 |\n",
    "|--------|--------|--------|-----------|\n",
    "| 低     | 否    | 好     | ？        |\n",
    "\n",
    "我们根据新的数据和决策树，首先判断学习时长，于是走L路线，然后判断是否复习，走H路线，随后判断睡眠质量，走G路线，最后得到的结果就直接是G边对应的叶子节点，也即Fail。于是我们就可以预测在这种情况下这位考生不能通过考试。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://mermaid.ink/img/Z3JhcGggVEQKICAgIEFbU3R1ZHlEdXJhdGlvbl0gLS0+fEh8IEIxW1Jldmlld2VkXQogICAgQSAtLT58TXwgQjJbUmV2aWV3ZWRdCiAgICBBIC0tPnxMfCBCM1tSZXZpZXdlZF0KICAgIEIxIC0tPnxZfCBDMVtTUV0KICAgIEIxIC0tPnxOfCBDMltTUV0KICAgIEIyIC0tPnxZfCBDM1tTUV0KICAgIEIyIC0tPnxOfCBDNFtTUV0KICAgIEIzIC0tPnxZfCBDNVtTUV0KICAgIEIzIC0tPnxOfCBDNltTUV0KICAgIEMxIC0tPnxHfCBEMVtQYXNzXQogICAgQzEgLS0+fEF8IEQyW1Bhc3NdCiAgICBDMSAtLT58UHwgRDNbRmFpbF0KICAgIEMyIC0tPnxHfCBENFtGYWlsXQogICAgQzIgLS0+fEF8IEQ1W0ZhaWxdCiAgICBDMiAtLT58UHwgRDZbRmFpbF0KICAgIEMzIC0tPnxHfCBEN1tGYWlsXQogICAgQzMgLS0+fEF8IEQ4W1Bhc3NdCiAgICBDMyAtLT58UHwgRDlbRmFpbF0KICAgIEM0IC0tPnxHfCBEMTBbRmFpbF0KICAgIEM0IC0tPnxBfCBEMTFbRmFpbF0KICAgIEM0IC0tPnxQfCBEMTJbRmFpbF0KICAgIEM1IC0tPnxHfCBEMTNbRmFpbF0KICAgIEM1IC0tPnxBfCBEMTRbUGFzc10KICAgIEM1IC0tPnxQfCBEMTVbRmFpbF0KICAgIEM2IC0tPnxHfCBEMTZbRmFpbF0KICAgIEM2IC0tPnxBfCBEMTdbUGFzc10KICAgIEM2IC0tPnxQfCBEMThbRmFpbF0KCiAgICBjbGFzc0RlZiByZWQgZmlsbDojZjY2LHN0cm9rZTojZjY2OwogICAgY2xhc3MgQjMsQzYsRDE2IHJlZDsKICAgIGxpbmtTdHlsZSAyIHN0cm9rZTojZjY2LHN0cm9rZS13aWR0aDoycHg7CiAgICBsaW5rU3R5bGUgOCBzdHJva2U6I2Y2NixzdHJva2Utd2lkdGg6MnB4OwogICAgbGlua1N0eWxlIDI0IHN0cm9rZTojZjY2LHN0cm9rZS13aWR0aDoycHg7\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with open('分类学习/决策树2.data', 'r', encoding='utf-8') as file:\n",
    "    mm(file.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ID3算法（第三代迭代二叉树算法）\n",
    "### 概述\n",
    "我们在实际的模型学习里当然不可能靠人的直觉来判断决策树应该以什么边和节点来构建，所以我们必须要有一个算法来代替直觉找到“最符合数据分布”的决策树。在这里，我们会使用ID3算法来进行处理。\n",
    "\n",
    "迭代二叉树算法的计算原理是对整个数据集进行“分裂”，也就是基于标签能提供的信息增益对整个数据集进行拆分。在选择出信息增益最大的标签之后，就会将数据集按这个标签进行分割，随后就像二叉树算法一样，对分割后的每一个子数据集重新执行这个算法，一直执行到所有标签全部被用来分割过一遍，或者达到预设的其他终止条件为止。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 熵 Entropy\n",
    "在上文中我们提到了**信息增益**这个关键词，在ID3算法里，信息增益指的是通过数据集的信息熵获得的。\n",
    "\n",
    "从直观上理解，信息熵可以这么解释：当一个数据集里元素分布得越均匀时，这个数据集的熵就越高。反之，如果这个数据集里元素分布得不均匀，那么这个数据集的熵就越低。极端情况是，如果一个数据集中所有元素都分布在同一个类别，那么这个数据集的熵就是0，也就是熵此时最低。\n",
    "\n",
    "通过对数据集中指定标签列进行分析，我们可以得到针对当前标签的数据集熵，计算公式如下：\n",
    "$$H(s)=-\\sum^{n}_{i=1}{\\left( p_i\\log_2{p_i} \\right)}$$\n",
    "其中：\n",
    " - $S$是数据集，$H(S)$是这个数据集的信息熵\n",
    " - $n$是标签中可取值的数量，比如上面睡眠质量有3种可能性，那么$n=3$，而是否复习有两种可能性，则$n=2$\n",
    " - $p_i$是数据集中第$i$类样本出现的概率\n",
    "   - 这里的概率实际上就只是出现频率，也就是当前分类出现的次数除以数据集大小，公式为$p_i=\\frac{|S_i|}{|S|}$\n",
    " - $\\log_{2}{p_i}$是样本概率的信息量，选择以2为底是因为信息论里我们使用bit作为信息单位，而bit能表示的组合有2种"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 信息增益 Information Gain\n",
    "光知道熵还不够，为了判断哪个标签可以提供“最多的信息”或者说哪个标签对结果影响最大，我们还需要知道“如果以这个标签对数据集进行分割，我们可以多知道多少信息”。这本质上也是将人的直觉进行量化，例如，当我们在屋内想知道今天是否下雨的时候，我们首先会看看天气预报，然后才会考虑拉开窗帘看看外面是不是真的下雨了。对决策树来说，这个过程是类似的，我们需要优先找出能提供最多信息的标签来对数据集进行分割\n",
    "\n",
    "当我们知道数据集的时候，也就可以对信息增益做出计算，公式如下：\n",
    "$$IG(A,S)=H(S)-\\sum_{t \\in T}{\\left( \\frac{|S_t|}{S}H(S_t) \\right)}$$\n",
    "其中:\n",
    " - $IG(A,S)$是在给定标签$A$下数据集$S$的信息增益，例如，当我们分析上面是否睡眠这个标签时，$A$就是是否睡眠这个标签，而$S$是整体数据集。当我们计算$IG(A,S)$时，我们本质上是在计算给定标签$A$可以给整体数据集$S$提供多少额外信息\n",
    " - $H(S)$是上文提到过的信息熵\n",
    " - $T$是所有可能的标签集合，例如，前文数据集的$T$实际上就是{学习时长, 是否复习, 睡眠质量}，而$t$则是$T$下的每一个可能取值\n",
    " - $S_t$指的是在我们选择$t$来划分时数据集$S$的信息增益\n",
    " - $\\frac{S_t}{S}$指的是当前分割后的子数据集占数据集整体的比例，我们需要这个值来对信息熵进行加权平均\n",
    "这整个公式实际含义是对按标签$A$对数据集$S$分割之后，每一个子数据集$S_t$的信息熵的总和相比$S$来说降低了多少，换言之就是量化这次分割可以给我们提供的额外信息。这一点很重要，因为它可以给我们提供一个量化的参考来选取最重要的标签进行分割。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 算法伪代码\n",
    "在我们知道信息熵和信息增益的计算方式后，就可以实际开始执行ID3算法了。算法的伪代码如下：\n",
    "```python\n",
    "ID3(S, Attr):\n",
    "    如果实例具有相同的标签：\n",
    "        返回一个单一节点的树，该节点的标签为这个共同的类标签\n",
    "    否则：\n",
    "        1. 创建一个根节点R\n",
    "        2. 使用信息增益确定增益最大的最佳分类属性A\n",
    "        3. 对属性A的每个可能值v：\n",
    "            a. 在树中为属性A添加一个对应其取值v的新的分支\n",
    "            b. 定义S_v为S中所有在属性A上取值为v的子集\n",
    "            c. 如果S_v为空：\n",
    "                在这个新分支下添加一个叶子节点，标记为S中最常见的标签，这样可以应对训练集中可能的缺省值\n",
    "            否则：\n",
    "                在这个新分支下递归调用ID3(S_v, Attr-A)\n",
    "        4. 返回根节点R\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
